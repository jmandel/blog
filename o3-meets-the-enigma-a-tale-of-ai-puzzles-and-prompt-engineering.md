---
title: "o3 Meets The Enigma: A Tale of AI, Puzzles, and Prompt Engineering"
date: 2025-01-01T00:00:00
slug: o3-meets-the-enigma-a-tale-of-ai-puzzles-and-prompt-engineering
banner: "https://media.licdn.com/mediaD5612AQGMXhAJMMIavQ"
---

<img alt="" src="https://media.licdn.com/mediaD5612AQGMXhAJMMIavQ" title=""/>
<h1><a href="https://www.linkedin.com/pulse/o3-meets-enigma-tale-ai-puzzles-prompt-engineering-josh-mandel-md-l0xec">o3 Meets The Enigma: A Tale of AI, Puzzles, and Prompt Engineering</a></h1>
<p class="created">Created on 2025-05-01 21:24</p>
<p class="published">Published on 2025-05-01 21:53</p>
<div><p>I accidentally hoarded OpenAI o3 query credits this month (the psychology of scarcity in the ChatGPT "Plus" tier!). Fortuitously, this left me with 50 queries to burn on testing o3's new vision and reasoning capabilities. The goal? A task I've tried on many vision language models over the years without success: <em>"Turn this image into clean JSON defining the grid structure."</em></p><p>Getting an AI to accurately transcribe a <strong>barred cryptic crossword grid</strong> has been surprisingly tough. These examples come from "The Enigma," the monthly puzzle bible from the National Puzzlers’ League (NPL – great org, <a href="https://www.puzzlers.org/" target="_blank"><strong>check them out</strong></a> if you love wordplay; e.g. see <a href="https://download.puzzlers.org/public/enigma-minisample.pdf" target="_blank">this mini-sample issue</a>.)</p><figure><img data-media-urn="urn:li:digitalmediaAsset:D5612AQGrmCcSEgoITQ" src="https://media.licdn.com/dms/image/v2/D5612AQGrmCcSEgoITQ/article-inline_image-shrink_1500_2232/B56ZaNw3ueGkAU-/0/1746135121408?e=1756944000&amp;v=beta&amp;t=l-ngKCnX1u-lbtBrP6UQuYEWK7_zkB_XD1zMIxJSumM"/><figcaption>A sample cryptic crossword grid from The Enigma </figcaption></figure><h3>Why Do Crossword Grids Challenge AI Vision?</h3><p>Unlike standard crosswords, barred grids don't use black squares. Every square gets a letter. You figure out where words end based on the <strong>thickness of the lines</strong> between cells. A thick 'bar' stops a word; a thin line doesn't. Add small clue numbers in the corners, and what looks like a simple regular structure to the human solver becomes a substantial hurdle for AI. Visually parsing the grid requires the ability to understand large- and small-scale structures, and to make hundreds of binary (thick-vs-thin) judgments without fail. </p><h3>Round 1: o3 Tries to Eyeball It (and Fails for 5 Minutes x5 tries)</h3><p>My first shots were direct: "o3, here's the grid image. Transcribe it to JSON, noting thick vs. thin bars." o3 spun its wheels for a good <strong>five minutes on average</strong>. You could see the gears turning – it was cropping the image, zooming in on sections, even writing and running Python snippets using image processing libraries to try and detect lines or analyze pixel density. It <em>knew</em> it was looking at a grid.</p><p>But the output was unusable:</p><ul><li><p><strong>Bad Dimensions:</strong> It hallucinated grid sizes, like guessing 15x15 for my sample 12x12 grid.</p></li><li><p><strong>Inconsistent Bars:</strong> The crucial thick-vs-thin distinction was a mess. It would get some right, some wrong, with no apparent pattern.</p></li><li><p><strong>Missing Numbers:</strong> Clue numbers were just ignored.</p></li><li><p><strong>Truncated JSON:</strong> It even gave up partway, literally outputting "... (eight more row-arrays omitted for brevity)..." which defeats the whole purpose!</p></li></ul><p>Clearly, even with its tool use, o3's direct visual analysis couldn't reliably nail the required precision for this specific task. It could <em>see</em>, but not accurately <em>interpret</em> the fine details consistently.</p><h3>Round 2: "Don't just do this one task; build a general-purpose solver!"</h3><p>This failure sparked a change in my prompting strategy. o3 is great at coding. What if I asked it to build a <em>program</em> to solve the transcription, instead of doing it directly? I refined the prompt:</p><blockquote><p>"Hint: your safest bet is to create a <strong>program</strong> that will perform this task reliably on <strong>any page</strong> from The Enigma... grids might have <strong>different dimensions</strong>. As you write the program, <strong>visually debug each step</strong>... then run it start to finish."</p></blockquote><p>I was explicitly telling it: don't trust your eyes alone; trust your code, and verify with your eyes. Build something robust.</p><h3>o3 Engineers the Solution</h3><p>That did the trick. o3 shifted focus to writing a Python script using OpenCV:</p><ol><li><p>It loaded the image and robustly found the grid boundaries.</p></li><li><p>It detected <em>all</em> horizontal and vertical lines.</p></li><li><p>Crucially, it implemented functions to <strong>measure the pixel thickness</strong> of each line segment, allowing it to reliably classify them as 'thick' or 'thin' based on a threshold.</p></li><li><p>For clue numbers, it smartly applied logic: instead of OCR, it <strong>calculated where numbers <em>should</em> be</strong> according to standard crossword rules (a word starts after a grid edge or a thick bar).</p></li><li><p>It packaged everything into a clean JSON structure with dimensions, bar locations (e.g., vertical_bars[row][col] indicating if the bar to the <em>right</em> of cell (r,c) is thick), and the derived numbers.</p></li></ol><p>And thanks to the "visually debug" instruction, it ran its own code, showed me intermediate outputs like overlays of detected bars, and refined the thresholds and logic as it went. It wasn't just coding; it was engineering a solution.</p><h3>Bringing the Tool to Life</h3><p>With the core logic proven in Python, I had o3 translate the whole thing into a standalone HTML/JavaScript web app using Canvas. A bit more iteration, and the result is a simple page where you can upload an image and get the analysis done right in your browser.</p><p><strong>Link to the web app: </strong><a href="https://chatgpt.com/canvas/shared/6812e9917874819197965ba56018198d" target="_blank"><strong>https://chatgpt.com/canvas/shared/6812e9917874819197965ba56018198d</strong></a></p><p>Just drop in a full Enigma page or a cropped grid (e.g., try the sample image above). The script locates the grid, filters out page noise, identifies bars, auto-numbers the cells, and gives you the JSON.</p><h3>Takeaways: o3, Prompts, and Precision</h3><p>This exercise was a great microcosm of working with advanced AI:</p><ul><li><p><strong>Reasoning &amp; Coding are Strengths:</strong> o3's ability to understand the goal, devise a multi-step algorithm, write complex code (Python/JS), use tools (code execution, image analysis), and debug is phenomenal.</p></li><li><p><strong>Vision Has Limits (for now):</strong> High-precision visual tasks requiring consistent interpretation of subtle geometric features are still challenging for direct analysis. o3 <em>knew</em> it was struggling, but couldn't overcome it visually.</p></li><li><p><strong>Meta-Prompting Works:</strong> Shifting the prompt from "do the task" to "build a tool for the task" allowed o3 to leverage its coding strength to bypass its visual weakness for this specific problem. That strategic redirection was key.</p></li><li><p><strong>AI as Tool-Builder, Not Just Task-Doer:</strong> The most effective use here wasn't asking o3 to <em>be</em> the grid transcriber, but to <em>create</em> the transcriber.</p></li></ul><p>While o3 needed specific guidance to nail this particular visual puzzle, its ability to then engineer a robust, algorithmic solution is incredibly promising. Give the detector tool a spin!</p><p></p><figure><img data-media-urn="urn:li:digitalmediaAsset:D5612AQG9AQiiS3D3zA" src="https://media.licdn.com/dms/image/v2/D5612AQG9AQiiS3D3zA/article-inline_image-shrink_1500_2232/B56ZaN1Ae2GUAU-/0/1746136205693?e=1756944000&amp;v=beta&amp;t=fiV51LidoPp37NtzvgxfCQQo6-Ovd5sN8C8KrTqpwCk"/><figcaption>Web app in action!</figcaption></figure><p></p></div>
